---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

Dr. Lianli Gao(È´òËÅî‰∏Ω) is a Professor in School of Computer Science and Engineering (CSE) at University of Electronic Science and Technology of China (UESTC). Dr. Gao received her Ph.D in School of Electronic Engineering and Computer Science (ITEE) at University of Queensland in 2014 (under the supervision of [Prof. Jane Hunter](https://staff.itee.uq.edu.au/jane/), [Prof. Michael Bruenig](https://researchers.uq.edu.au/researcher/5052) and [A/Prof. Yuan-Fang Li](https://users.monash.edu.au/~yli/) and her B.CS. in School of Computer Science and Engineering (CSE) at University of Electronic Science and Technology of China (UESTC) in 2009, respectively.

Dr. Gao is the leader of the  Multimedia Analysis and Visual Cognition research group. She has a strong ability to develop and author successful grant funding proposals in close collaboration with industry, government partners, and colleagues within and across universities. Her research has been supported by 12 nationally competitive research grants, including one Major Project from the Ministry of Science and Technology of China, two key projects from the National Natural Science Foundation of China, four projects from the industry, etc. In addition, she has served or will serve as ICCV Area Chair, CVPR Area Chair, ECCV Area Chair, WACV Area Chair, AAAI Area Chair, ACM MM Area Chair, program committee of the IJCAI 24 track on AI and Social Good, ACM MM Session Chair, ACM MM 2021 Workshop Co-chair, IJCAI Session Chair, Guest Editor of 2019 Journal of Visual Communication and Image Representation, etc.

Her research interests include multimedia understanding, computer vision,  artificial intelligence (AI), machine learning, and AI for Robotics, and published over 160 publications at prestigious journals and proceedings in prominent conferences (including 70+ IEEE/ACM Transactions and 70+ CCF-A papers (Chinese Computing Federation A ranked (e.g., CVPR, ICCV, NeurIPS, ICML, and ICLR)). Her publications have been cited in Google Scholar more than 10,000+ times, and her H-Index in Google Scholar is 54. She has received Best Student Paper Award from Australasian Database Conference 2017, Rising Star Award from IEEE Technical Community on Multimedia Computing 2020, Sichuan Provincial Academic/Technical Leader (Reserve Candidate) 2021, UESTC Research Excellence Award (2018, 2020,2023), Alibaba DAMO Academy Young Fellow Award 2019, Rising Star of Science Award 2023, and also has been selected as one of the 2023 Chinese Young Female Scholars in Artificial Intelligence for her outstanding academic performance in AI. In terms of international challenges she received ICCV  Deeper Action 3rd Place Award in Kinetics-TPS Challenge on Part-level Action Parsing 2021, CVPR Security AI Challenger Phrase VI Track 1st Place award in White-box Adversarial Attacks on ML Defense Models 2021, ICCV COCO DensePose Challenge 2nd place award 2019, OPPO Security Challenge 2nd Place 2021, and ECCV DeeperAction Track4 3nd Place 2022, etc. 

<!-- 
Her Teaching covers a wide range of courses at different levels for both international and national students, including "Introduction to Big Data", "Data Structure and Algorithm", "Introduction to Computer Vision", "Semantic Web", "Experiments of Artificial Intelligence", "Welcome to Academic World" etc. She received very positive feedback (e.g., "nice teacher", "well-organized", "good experience" and "dedicated to providing us with knowledge‚Äú) from her students with a well-deserved 5-star rating. In addition, she has been mentoring junior staff by assessing peer-to-peer teaching performance via attending their classes. Her dedication earned her Excellent Faculty Award for Outstanding Teaching in 2018 and 2023.
-->

# üî• Hire
 We consistently have open positions available for Professors, Associate Professors, Lecturers, Postdocs, and PhD students. If you are interested Feel free to reach out to me via email.
  
# üî• News
<!-- - *2022.02*: &nbsp;üéâüéâ Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  -->
<!-- -
- *2025.02*: &nbsp; Board member of Tibet Science and Technology Awards and Recognition (Serviced as: Vice-Chair of Electronic Science and Information Science Track 2025)   -->
- *2025.05*: &nbsp; My research has been cited over 10,000 times on Google Scholar!!
- *2025.05*: &nbsp; One Paper accepted by ACL 2025! congratulations to Haonan Zhang!!
- *2025.03*: &nbsp; One Paper accepted by CVPR 2025! congratulations to Shihan Wu and Ji Zhang!!
- *2025.02*: &nbsp; One Paper accepted by IEEE TIP 2025! congratulations to Xingyu Lv!!
- *2025.01*: &nbsp; One Paper accepted by IJCV 2025!!! A good start on 2025!! congratulations to Xingyu Lv!!
- *2025.01*: &nbsp; One Best Paper Award from Sichuan Province Computer Federation 2025! congratulations to Ji Zhang!!
- *2025.01*: &nbsp; One Best Paper Honorable Mention Award from Sichuan Province Computer Federation 2025! congratulations Kaipeng Fang!!
- *2025.01*: &nbsp; One paper is accepted by IEEE TMM 2025! congratulations to LianQiang Gan!!
- *2024.12*: &nbsp; One paper is accepted by AAAI 2025!
- *2024.11*: &nbsp; First Place Award in the Integrated Media Content Creator category, China Mobile AI Marathon Innovation! Congratulations to our AIGC Group!!
- *2024.10*: &nbsp; Two papers accepted by NeurIPS 2024!
- *2024.09*: &nbsp; appointed as an Associate Editor for IEEE Transactions on Multimedia 2024!
- *2024.08*: &nbsp; Best Paper Candidate of ICME 2024!
- *2024.07*: &nbsp; Two papers were accepted by ACM MM 2024!
- *2024.02*: &nbsp; Two papers were accepted by IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2024)!
- *2024.01*: &nbsp; Four papers were accepted by IEEE Transactions on Multimedia (TMM 2024)!
- *2023.10*: &nbsp; One paper was accepted by IEEE Transactions on Image Processing (TIP 2023)!
- *2023.09*: &nbsp; One paper was accepted by Annual Conference on Neural Information Processing Systems (NeurIPS 2023)!
- *2023.07*: &nbsp; Four papers were accepted by ACM Multimedia (MM 2023)!
- *2023.07*: &nbsp; One paper was accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI 2023)!
- *2023.05*: &nbsp; Two papers were accepted by IEEE/CVF International Conference on Computer Vision (ICCV 2023)!
- *2023.03*: &nbsp; One paper was accepted by IEEE Transactions on Image Processing (TIP 2023)!
- *2023.02*: &nbsp; One paper was accepted by IEEE Transactions on Image Processing (TIP 2023)!


# üìù Publications 
Here are some **selective publications**. For **full publications**, please visit my [Google Scholar](https://scholar.google.com/citations?user=zsm2dpYAAAAJ) and [DBLP](https://dblp.org/pid/123/9849.html).

<div class='paper-box'><div class='paper-box-image'><div><div class="badge"> 2025 Robotic AI </div><img src='images/papers/InSpire.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**InSpire: Vision-Language-Action Models with Intrinsic Spatial Reasoning.**](https://arxiv.org/abs/2505.13888) \\
J. Zhang, S. Wu, X. Luo, H. Wu, **Lianli Gao**,  H. T. Shen, J. Song\\
<a href="https://arxiv.org/pdf/2505.13888"><strong>Paper</strong></a>
\|
<a href="https://koorye.github.io/proj/Inspire/"><strong>Project</strong></a>

** a simple yet effective approach that mitigates the adverse effects of spurious correlations by boosting the spatial reasoning ability of VLAs.**
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge"> 2025 Robotic AI </div><img src='images/papers/PCD.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**Policy Contrastive Decoding for Robotic Foundation Models.**](https://arxiv.org/abs/2505.13255) \\
S. Wu, J. Zhang, X. Luo,  J. Xie, J. Song,  and H. T. Shen,  **Lianli Gao**\\
<a href="https://arxiv.org/abs/2505.13255"><strong>Paper</strong></a>
\|
<a href="https://koorye.github.io/proj/PCD/"><strong>Project</strong></a>

** a novel Policy Contrastive Decoding for Robotic Foundation Models .**
</div>
</div>






<div class='paper-box'><div class='paper-box-image'><div><div class="badge"> 2025 Text-to-3D Benchmark</div><img src='images/papers/T23D.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**GT23D-Bench: A Comprehensive General Text-to-3D Generation Benchmark.**](https://arxiv.org/pdf/2412.09997) \\
S. Su, X. Cai, **Lianli Gao**, P. Zeng, Q. Du, M. Li, H. T. Shen, and J. Song.\\
<a href="https://arxiv.org/pdf/2309.07439.pdf"><strong>Paper</strong></a>
\|
<a href="https://gt23d-bench.github.io/"><strong>Project</strong></a>

**A Benchmark for general Text-to-3D Generation.**
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/papers/Nips2024-halluciation.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
  
[**Dept: Decoupled prompt tuning.**](https://arxiv.org/pdf/2309.07439.pdf) \\
J. Zhang, S. Wu, **Lianli Gao**, H. T. Shen, and J. Song.\\
_IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (**CVPR**), 2024\\
 <a href="https://ieeexplore.ieee,org/stamp/stamp.jsp?arnumber=9969654"><strong>Paper</strong></a>
\|
<a href="https://github.com/Koorye/DePT"><strong>Code</strong></a>


**Overcoming base-new trade-off problem for existing prompt tuning methods.**

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/papers/2.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**Prog: Prompting-to simulate generalized knowledge for universal cross-domain retrieval.**](https://arxiv.org/pdf/2312.12478.pdf) \\
K. Fang, J. Song, **Lianli Gao**, P. Zeng, Z.-Q. Cheng, X. Li, and H. T. Shen. \\
_IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (**CVPR**), 2024\\
 <a href="https://arxiv.org/pdf/2312.12478.pdf"><strong>Paper</strong></a>
\|
<a href="https://github.com/fangkaipeng/ProS"><strong>Code</strong></a>


**Applying prompt tuning to produce generalized features for UCDR.**

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><img src='images/papers/Nips2024-Coin.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
  
[**CoIN: A Benchmark of Continual Instruction tuNing for Multimodel Large Language Model.**](https://arxiv.org/abs/2403.08350) \\
C. Chen, J. Zhu, X. Luo, H. T. Shen, **Lianli Gao**, J. Song. \\
_Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems_ (**NeurIPS**), 2024\\
<a href="https://arxiv.org/abs/2403.08350"> <strong>Paper</strong></a>
\|
<a href="https://github.com/zackschen/CoIN"><strong>Code</strong></a> 

**Proposing a benchmark of Continual Instruction tuNing for MLLMs .**

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><img src='images/papers/Nips2024-halluciation.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
  
[**Alleviating Hallucinations in Large Vision-Language Models through Hallucination-Induced Optimization.**](https://arxiv.org/pdf/2405.15356) \\
B. Chen, X. Lyu, **Lianli Gao**, J. Song and H. T. Shen. \\
_Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems_ (**NeurIPS**), 2024\\
<a href="https://arxiv.org/pdf/2405.15356"> <strong>Paper</strong></a>
\|
<a href="https://github.com/BT-C/HIO"><strong>Code</strong></a> 


**Alleviating Hallucinations in LVLMs.**

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2024</div><img src='images/papers/ECCV2024-GAKer.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
 
[**Any Target Can be Offense: Adversarial ExampleGeneration via Generalized Latent Infection.**](https://arxiv.org/pdf/2407.12292) \\
Y. Sun, S. Yuan, X. Wang, **Lianli Gao**, J. Song. \\
_European Conference on Computer Vision_ (**ECCV**), 2024\\
<a href="https://arxiv.org/pdf/2407.12292"> <strong>Paper</strong></a>
\|
<a href="https://github.com/VL-Group/GAKer"><strong>Code</strong></a> 

**Proposing a method enabling construct adversarial examples to any target class.**

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TIP 2024</div><img src='images/papers/TIP2024-CPI.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
 
[**CPI-Parser: Integrating Causal Properties Into Multiple Human Parsing.**](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10704987) \\
X. Wang, X. Chen, **Lianli Gao**, J. Song, H. T. Shen. \\
_IEEE Trans. Image Process._ (**TIP**), 33:5771-5782, 2024\\
<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10704987"> <strong>Paper</strong></a>
\|
<a href="https://github.com/HAG-uestc/CPI-Parser"><strong>Code</strong></a> 

**Proposing a Causal-based method for Human Parsing tasks.**

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IJCV 2024</div><img src='images/papers/ijcv.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**Informative scene graph generation via debiasing.**](https://arxiv.org/pdf/2308.05286.pdf) \\
 **Lianli Gao**, X. Lyu, Y. Guo, Y. Hu, Y.-F. Li, L. Xu, H. T. Shen, and J. Song.
_ArXiv_ (****), 2024\\
 <a href="https://arxiv.org/pdf/2308.05286.pdf"><strong>Paper</strong></a>
\|
<a href="https://github.com/ZhuGeKongKong/SGG-G2S"><strong>Code</strong></a>


**Making balanced and informative predicate prediction for SGG.**

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TPAMI 2023</div><img src='images/papers/4.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**Label-guided generative adversarial network for realistic image synthesis.**](https://users.monash.edu/~yli/assets/pdf/img_syn_tpami2022.pdf) \\
J. Zhu, **Lianli Gao**, J. Song, Y. Li, F. Zheng, X. Li, and H. T. Shen.\\
_IEEE Trans. Pattern Anal. Mach. Intell._ (**TPAMI**), 45(3):3311‚Äì3328, 2023\\
 <a href="https://users.monash.edu/~yli/assets/pdf/img_syn_tpami2022.pdf"><strong>Paper</strong></a>
\|
<a href="https://github.com/RoseRollZhu/Lab2Pix-V2"><strong>Code</strong></a>
 
**Bridging semantic gap between labels and images for Label-Image Generation.**

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TPAMI 2023</div><img src='images/papers/tpami.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**Adaptive fine-grained predicates learning for scene graph generation.**](https://arxiv.org/pdf/2207.04602.pdf) \\
X. Lyu, **Lianli Gao**, P. Zeng, H. T. Shen, and J. Song.\\
_IEEE Trans. Pattern Anal. Mach. Intell._ (**TPAMI**), 45(11):13921‚Äì13940, 2023\\
 <a href="https://arxiv.org/pdf/2207.04602.pdf"><strong>Paper</strong></a>
\|
<a href="https://github.com/XinyuLyu/FGPL"><strong>Code</strong></a>

**Ensuring balanced and efficient learning process for fine-grained SGG.**
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICML 2023</div><img src='images/papers/5.jpeg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**A closer look at few-shot classification again.**](https://arxiv.org/pdf/2301.12246.pdf) \\
 X. Luo, H. Wu, J. Zhang, **Lianli Gao**, J. Xu, and J. Song.\\
_International Conference on Machine Learning_ (**ICML**), pages 23103‚Äì23123, 2023\\
<a href="https://arxiv.org/pdf/2301.12246.pdf"><strong>Paper</strong></a>
\|
<a href="https://github.com/Frankluox/CloserLookAgainFewShot"><strong>Code</strong></a>

**Empirically proving disentanglement of training and test-time adaptation algorithms in FSL.**

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2023</div><img src='images/papers/cvpr23.png' alt="sym" width="100%" height="125%"></div></div>
<div class='paper-box-text' markdown="1">

[**Prototype-based Embedding Network for Scene Graph Generation.**](https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_Prototype-Based_Embedding_Network_for_Scene_Graph_Generation_CVPR_2023_paper.pdf) \\
C. Zheng, X. Lyu, **Lianli Gao**, B. Dai, and J. Song.\\
_IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (**CVPR**), pages 22783‚Äì22792, 2023\\
<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_Prototype-Based_Embedding_Network_for_Scene_Graph_Generation_CVPR_2023_paper.pdf"><strong>Paper</strong></a>
\|
<a href="https://github.com/VL-Group/PENET"><strong>Code</strong></a>

**Acquiring robust features for reliable relation prediction in SGG.**

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2023</div><img src='images/papers/7.png' alt="sym" width="100%" height="125%"></div></div>
<div class='paper-box-text' markdown="1">

[**Part-aware transformer for generalizable person re-identification.**](https://openaccess.thecvf.com/content/ICCV2023/papers/Ni_Part-Aware_Transformer_for_Generalizable_Person_Re-identification_ICCV_2023_paper.pdf) \\
H. Ni, Y. Li, **Lianli Gao**, H. T. Shen, and J. Song.\\
_IEEE/CVF International Conference on Computer Vision_ (**ICCV**), pages 11246‚Äì11255, 2023\\
<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ni_Part-Aware_Transformer_for_Generalizable_Person_Re-identification_ICCV_2023_paper.pdf"><strong>Paper</strong></a>
\|
<a href="https://github.com/liyuke65535/Part-Aware-Transformer"><strong>Code</strong></a>

**Mitagating domain-specific biases in Domain generalization person ReID.**

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TIP 2023</div><img src='images/papers/6.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**Toward a unified transformer-based framework for scene graph generation and human-obfect interaction detection.**](https://arxiv.org/pdf/2311.01755.pdf)  \\
T. He, **Lianli Gao**, J. Song, and Y. Li. \\
_IEEE Trans. Image Process._ (**TIP**), 32:6274‚Äì6288, 2023\\
<a href="https://arxiv.org/pdf/2311.01755.pdf"><strong>Paper</strong></a>
\|
<a href="https://lianligao,github.io/"><strong>Code</strong></a>

**Build a Unified Transformer-based Framework for SGG and HOI.**

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">MM 2023</div><img src='images/papers/8.png' alt="sym" height="600" width="800"></div></div>
<div class='paper-box-text' markdown="1">

[**Moviefactory: Automatic movie creation from text using large generative models for language and images.**](https://arxiv.org/pdf/2306.07257.pdf) \\
 J. Zhu, H. Yang, H. He, W.Wang, Z. Tuo, W. Cheng, **Lianli Gao**, J. Song, and J. Fu. \\
_Proceedings of the 31st ACM International Conference on Multimedia_ (**ACM MM**), pages 9313‚Äì9319, 2023\\
<a href="https://arxiv.org/pdf/2306.07257.pdf"><strong>Paper</strong></a>
\|
<a href="https://www.youtube.com/watch?v=tvDknhMFhzk"><strong>Demo</strong></a>

**Empowering users to create captivating movies using simple text inputs.**

</div>
</div>


- `TCSVT 2024` [Ump: Unified Modality-Aware Prompt Tuning for Text-Video Retrieval.]([https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10599510](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10599510)) H. Zhang, P Zeng, **Lianli Gao**, J. Song, and H. Shen. _IEEE Trans. Circuits Syst. Video Technol.,_ 34(11):11954 - 11964, 2024. [Code](https://github.com/zchoi/UMP_TVR)
- `TCSVT 2024` [Dual-Branch Hybrid Learning Network for Unbiased Scene Graph Generation.]([https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10189863](https://arxiv.org/pdf/2207.07913)) C. Zheng, **Lianli Gao**, X. Lyu, P Zeng, A Saddik, and H. Shen. _IEEE Trans. Circuits Syst. Video Technol.,_ 34(3):1743 - 1756, 2024. [Code](https://github.com/aa200647963/SGG-DHL/)
- `TMM 2024` [Exploring Spatial Frequency Information for Enhanced Video Prediction Quality.]([https://ieeexplore.ieee.org/abstract/document/10487871](https://ieeexplore.ieee.org/abstract/document/10487871)) J. Lai, L. Gan, J. Zhu, H. Liu, and **Lianli Gao**. _IEEE Trans. Multim.,_ 26:8955 - 8968, 2024. [Code](https://github.com/LintureGrant2023/SDFNet)
- `AAAI 2024` [F¬≥-Pruning: A Training-Free and Generalized Pruning Strategy towards Faster and Finer Text-to-Video Synthesis.]([https://ojs.aaai.org/index.php/AAAI/article/download/28300/28590](https://ojs.aaai.org/index.php/AAAI/article/download/28300/28590)) S. Su, J. Liu, **Lianli Gao**, and J Song. _Proceedings of the AAAI Conference on Artificial Intelligence,_ 38(5), 4961 - 4969, 2024. [Code](https://lianligao.github.io/)
- `ACMMM 2024` [SI-BiViT: Binarizing Vision Transformers with Spatial Interaction.]([https://dl.acm.org/doi/10.1145/3664647.3680872](https://dl.acm.org/doi/10.1145/3664647.3680872)) P. Yin, X. Zhu, J. Song, **Lianli Gao**, HT. Shen. _Proceedings of the 32nd ACM International Conference on Multimedia,_ 8169-8178, 2024. [Code](https://github.com/VL-Group/SI-BiViT)
- `ACMMM 2024` [MagicVFX: Visual Effects Synthesis in Just Minutes.]([https://openreview.net/pdf?id=GjmjOCYntQ](https://openreview.net/pdf?id=GjmjOCYntQ)) J Guo, **Lianli Gao**, J Zhu, J Zhang, S Li, J Song. _Proceedings of the 32nd ACM International Conference on Multimedia,_ 8238-8246, 2024. [Code](https://github.com/ruffiann/MagicVFX)
- `ACMMM 2024` [MPT: Multi-grained Prompt Tuning for Text-Video Retrieval.]([https://openreview.net/pdf?id=GjmjOCYntQ](https://openreview.net/pdf?id=GjmjOCYntQ)) H. Zhang, P. Zeng, **Lianli Gao**, J. Song, HT. Shen. _Proceedings of the 32nd ACM International Conference on Multimedia,_ 1206-1214, 2024. [Code](https://github.com/zchoi/MPT)
- `NeurIPS 2023` [Prototype-based Aleatoric Uncertainty Quantification for Cross-modal Retrieval.]([https://proceedings.neurips.cc/paper_files/paper/2023/file/4d893f766ab60e5337659b9e71883af4-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/4d893f766ab60e5337659b9e71883af4-Paper-Conference.pdf)) L. Hao, J. Song, **Lianli Gao**, X. Zhu, and H. Shen. _Advances in Neural Information Processing Systems, NeurIPS,_ 2023. [Code](https://github.com/leolee99/PAU)
- `ICCV 2023` [DETA: denoised task adaptation for few-shot learning.](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_DETA_Denoised_Task_Adaptation_for_Few-Shot_Learning_ICCV_2023_paper.pdf) J. Zhang, **Lianli Gao**, X. Luo, H. Shen, and J. Song. _IEEE/CVF International Conference on Computer Vision, ICCV,_ pages 11507‚Äì11517, 2023. [Code](https://github.com/JimZAI/DETA)
- `ACMMM 2023` [Precise Target-Oriented Attack against Deep Hashing-based Retrieval.]([https://dl.acm.org/doi/abs/10.1145/3581783.3612364](https://dl.acm.org/doi/abs/10.1145/3581783.3612364)) W. Zhao, J. Song, S. Yuan, **Lianli Gao**, Y. Yang, HT. Shen. _Proceedings of the 31st ACM International Conference on Multimedia,_ 6379-6389, 2024. [Code](https://github.com/1nAzureSky/Precise-Target-Oriented-Attack-against-Deep-Hashing-based-Retrieval)
- `ACMMM 2023` [Depth-aware sparse transformer for video-language learning.]([https://dl.acm.org/doi/abs/10.1145/3581783.3611714](https://dl.acm.org/doi/abs/10.1145/3581783.3611714)) H. Zhang,  **Lianli Gao**, P. Zeng, A. Hanjalic, HT. Shen. _Proceedings of the 31st ACM International Conference on Multimedia,_ 4778-4787, 2024. [Code](https://github.com/zchoi/DAST)
- `ACMMM 2023` [CUCL: Codebook for Unsupervised Continual Learning.]([https://dl.acm.org/doi/abs/10.1145/3581783.3611713](https://dl.acm.org/doi/abs/10.1145/3581783.3611713)) C. Cheng, J. Song, X. Zhu, J. Zhu, **Lianli Gao**, HT. Shen. _Proceedings of the 31st ACM International Conference on Multimedia,_ 1729-1737, 2024. [Code](https://github.com/zackschen/CUCL)
- `ACMMM 2023` [Mobilevidfactory: Automatic diffusion-based social media video generation for mobile devices from text.]([https://arxiv.org/pdf/2307.16371](https://arxiv.org/pdf/2307.16371)) J. Zhu, H. Yang, W. Wang, H. He, Z. Tuo, Y. Yu, WH. Cheng, **Lianli Gao**, J. Song, J. Fu, J. Luo. _Proceedings of the 31st ACM International Conference on Multimedia,_ 9313-9319, 2024. [Code](https://lianligao.github.io/)
- `TIP 2023` [Toward a Unified Transformer-Based Framework for Scene Graph Generation and Human-Object Interaction Detection.]([https://ieeexplore.ieee.org/document/10315051](https://ieeexplore.ieee.org/document/10315051)) T. He, **Lianli Gao**, J. Song, YF. Li. _IEEE Trans. Image Process.,_ 32:6274 - 6288, 2023. [Code](https://github.com/Koorye/DePT)
- `TIP 2023` [Revisiting multi-codebook quantization.]([https://ieeexplore.ieee.org/document/10087307](https://ieeexplore.ieee.org/document/10087307)) X. Zhu, J. Song, **Lianli Gao**, X. Gu, HT. Shen
. _IEEE Trans. Image Process.,_ 32:2399 - 2412, 2023. [Code](https://github.com/DeepMCQ/DeepQ)
- `TIP 2023` [From global to local: Multi-scale out-of-distribution detection.]([https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9969654](https://arxiv.org/pdf/2308.10239.pdf)) J. Zhang, **Lianli Gao**, B. Hao, H. Huang, J. Song, and H. Shen. _IEEE Trans. Image Process.,_ 32:6115‚Äì6128, 2023. [Code](https://github.com/Koorye/DePT)
- `TNNLS 2023` [Overcoming Data Deficiency for Multi-Person Pose Estimation.]([https://arxiv.org/pdf/2211.09469](https://arxiv.org/pdf/2211.09469)) Y. Dai, X. Wang, **Lianli Gao**, J. Song, F. Zheng, HT. Shen. _IEEE Trans. Neural Networks Learn. Syst._, 35(8):10857 - 10868, 2023. [Code](https://lianligao.github.io/)
- `TMM 2023` [Resparser: Fully convolutional multiple human parsing with representative sets.]([https://arxiv.org/pdf/2211.09469](https://arxiv.org/pdf/2211.09469)) Y Dai, X Chen, X Wang, M Pang, **Lianli Gao**, HT Shen. _IEEE Trans. Neural Networks Learn. Syst._, 26(5), 1384 - 1394, 2023. [Code](https://lianligao.github.io/)
- `TMM 2023` [Memory-based augmentation network for video captioning.]([https://ieeexplore.ieee.org/abstract/document/10487871](https://ieeexplore.ieee.org/abstract/document/10487871)) S. Jing, H. Zhang, P. Zeng, **Lianli Gao**, J. Song, HT. Shen. _IEEE Trans. Multim.,_ 26:2367 - 2379, 2023. [Code](https://lianligao.github.io/)
- `TMM 2023` [DMH-CL: Dynamic Model Hardness Based Curriculum Learning for Complex Pose Estimation.]([https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10227611](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10227611)) Y. Dai, B. Chen, **Lianli Gao**, J. Song, HT. Shen. _IEEE Trans. Multim.,_ 26:2367 - 2379, 2023. [Code](https://lianligao.github.io/)
- `TMM 2023` [Utilizing greedy nature for multimodal conditional image synthesis in transformers.]([https://ieeexplore.ieee.org/abstract/document/10184483](https://ieeexplore.ieee.org/abstract/document/10184483)) S. Su, J. Zhu, **Lianli Gao**, J. Song. _IEEE Trans. Multim.,_ 26:2354 - 2366, 2023. [Code](https://lianligao.github.io/)
- `TCSVT 2023` [Allowing Supervision in Unsupervised Deformable-Instances Image-to-Image Translation.]([https://ieeexplore.ieee.org/abstract/document/10363214/](https://ieeexplore.ieee.org/abstract/document/10363214/)) Y. Liu, S. Su, J. Zhu, F. Zheng, **Lianli Gao**, J. Song. _IEEE Trans. Circuits Syst. Video Technol.,_ 34(3):1743 - 1756, 2023. [Code](https://lianligao.github.io/)
- `TCSVT 2023` [SPT: Spatial pyramid transformer for image captioning.]([https://ieeexplore.ieee.org/abstract/document/10363214/](https://ieeexplore.ieee.org/abstract/document/10363214/)) H. Zhang, P. Zeng, **Lianli Gao**, X. Lyu, J. Song, HT. Shen. _IEEE Trans. Circuits Syst. Video Technol.,_ 34(6):4829 - 4842, 2023. [Code](https://lianligao.github.io/)
- `TCSVT 2023` [Complementarity-aware space learning for video-text retrieval.]([\https://ieeexplore.ieee.org/abstract/document/10012341](https://ieeexplore.ieee.org/abstract/document/10012341)) J. Zhu, P. Zeng, **Lianli Gao**, G. Li, D. Liao, J Song. _IEEE Trans. Circuits Syst. Video Technol.,_ 33(8), 4362-4374, 2023. [Code](https://lianligao.github.io/)
- `TIP 2022` [Hierarchical representation network with auxiliary tasks for video captioning and video question answering.](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9592722) **Lianli Gao**, Y. Lei, P. Zeng, J. Song, M. Wang, and H. T. Shen. _IEEE Trans. Image Process._, 31:202‚Äì215, 2022. [Code](https://github.com/riesling00/HRNAT)
- `TIP 2022` [Video question answering with prior knowledge and object-sensitive learning.](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9882977) P. Zeng, H. Zhang, **Lianli Gao**, J. Song, and H. T. Shen. _IEEE Trans. Image Process.,_ 31:5936‚Äì5948, 2022. [Code](https://github.com/zchoi/PKOL)
- `ECCV 2022` [State-aware compositional learning toward unbiased training for scene graph generation.](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9969654) T. He, **Lianli Gao**, J. Song, and Y. Li. _IEEE Trans. Image Process.,_ 32:43‚Äì56, 2023. [Code](https://lianligao.github.io/)
- `CVPR 2022` [Practical evaluation of adversarial robustness via adaptive auto attack.](https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Practical_Evaluation_of_Adversarial_Robustness_via_Adaptive_Auto_Attack_CVPR_2022_paper.pdf) Y. Liu, Y. Cheng, **Lianli Gao**, X. Liu, Q. Zhang, and J. Song. _IEEE/CVF Conference on Computer Vision and PatternRecognition, CVPR,_ pages 15084‚Äì15093, 2022. [Code](https://github.com/liuye6666/adaptive)
- `CVPR 2022` [Unified multivariate gaussian mixture for efficient neural image compression.](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhu_Unified_Multivariate_Gaussian_Mixture_for_Efficient_Neural_Image_Compression_CVPR_2022_paper.pdf) X. Zhu, J. Song, **Lianli Gao**, F. Zheng, and H. T. Shen. _IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR_, pages 17591‚Äì17600, 2022. [Code](https://github.com/xiaosu-zhu/McQuic)
- `CVPR 2022` [Fine-grained predicates learning for scene graph generation.](https://openaccess.thecvf.com/content/CVPR2022/papers/Lyu_Fine-Grained_Predicates_Learning_for_Scene_Graph_Generation_CVPR_2022_paper.pdf) X. Lyu, **Lianli Gao**, Y. Guo, Z. Zhao, H. Huang, H. T. Shen, and J. Song. _IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR,_ pages 19445‚Äì19453, 2022. [Code](https://github.com/XinyuLyu/FGPL)
- `ECCV 2022` [Towards open-vocabulary scene graph generation with prompt-based finetuning.](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136880055.pdf) T. He, **Lianli Gao**, J. Song, and Y. Li.  _IEEE/CVF International Conference on Computer Vision, ICCV,_ pages 56‚Äì73, 2022. [Code](https://lianligao.github.io/)
- `ECCV 2022` [Frequency domain model augmentation for adversarial attack.](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640543.pdf) Long, Q. Zhang, B. Zeng, **Lianli Gao**, X. Liu, J. Zhang, and J. Song. _IEEE/CVF International Conference on Computer Vision, ICCV,_ pages 549‚Äì566, 2022. [Code](https://github.com/yuyang-long/SSA)
- `ICLR 2022` [Beyond imagenet attack: Towards crafting adversarial examples for black-box domains.](https://arxiv.org/pdf/2201.11528.pdf) Q. Zhang, X. Li, Y. Chen, J. Song, **Lianli Gao**, Y. He, and H. Xue.  _The Tenth International Conference on Learning Representations, ICLR,_ 2022. [Code](https://github.com/Alibaba-AAIG/Beyond-ImageNet-Attack)
- `NeurIPS 2022` [A differentiable semantic metric approximation in probabilistic embedding for cross-modal retrieval.](https://proceedings.neurips.cc/paper_files/paper/2022/file/4e786a87e7ae249de2b1aeaf5d8fde82-Paper-Conference.pdf) H. Li, J. Song, **Lianli Gao**, P. Zeng, H. Zhang, and G. Li.  _Advances in Neural Information Processing Systems, NeurIPS,_ 2022. [Code](https://github.com/VL-Group/2022-NeurIPS-DAA)
- `NeurIPS 2022` [Natural color fool: Towards boosting black-box unrestricted attacks.](https://arxiv.org/pdf/2210.02041.pdf) S. Yuan, Q. Zhang, **Lianli Gao**, Y. Cheng, and J. Song. _Advances in Neural Information Processing Systems, NeurIPS,_ 2022. [Code](https://github.com/VL-Group/Natural-Color-Fool)
- `NeurIPS 2022` [A lower bound of hash codes‚Äô performance.](https://arxiv.org/pdf/2210.05899.pdf) X. Zhu, J. Song, Y. Lei, **Lianli Gao**, and H. Shen. _Advances in Neural Information Processing Systems, NeurIPS,_ 2022. [Code](https://github.com/vl-group/lbhash)
- `TPAMI 2020` [Hierarchical lstms with adaptive attention for visual captioning.](https://arxiv.org/pdf/1812.11004.pdf) **Lianli Gao**, X. Li, J. Song, and H. T. Shen. _IEEE Trans. Pattern Anal. Mach. Intell.,_ 42(5):1112‚Äì1131, 2020. [Code](https://github.com/lixiangpengcs/Spatial-Temporal-Adaptive-Attention-for-Video-Captioning)
<!-- under review -->

# üéñ Honors and Services
- **Research Honors:**
  - *2023, 2024* Rising Star of Science Award.
  - *2023*, *2020*, *2018* UESTC Research Excellence Award.
  - *2023*, *2018* UESTC Excellent Faculty Award for Teaching Excellence.
  - *2023* Chinese Young Female Scholars in Artificial Intelligence.
  - *2021* Sichuan Provincial Academic/Technical Leader (Reserve Candidate).
<!--   - *2020* IEEE Transactions on Multimedia Best Paper Award. -->
  - *2020* IEEE Technical Community on Multimedia Computing Rising Star Award.
  - *2019* Alibaba DAMO Academy Young Fellow Award.
  - *2017* Australasian Database Conference Best Student Paper Award.
- **Grand Challenges:**
  - *ECCV 2022:* DeeperAction Challenge 3rd place award on Track 4 Kinetics-TPS Challenge on Part-level Action Parsing.
  - *CVPR 2021:* Security AI Challenger Phrase VI Track 1st Place award in White-box Adversarial Attacks on ML Defense Models.
  - *ICCV 2021:* DeeperAction Challenge 3rd Place award on Track 3 Kinetics-TPS Challenge on Part-level Action Parsing.
  - *OPPO 2021:* Security Challenge 2nd Place award.
  - *ICCV 2019:* COCO DensePose Task Challenge 2nd place award.
- **Academic Services:**
    *To date:* ICCV Area Chair, CVPR Area Chair, ECCV Area Chair, WACV Area Chair, AAAI Area Chair, ACM MM Area Chair, etc.
  - *2025:* Senior program committee of AAAI 2025, ICME Area Chair, AAAI Area Chair, CVPR Area Chair, ICCV Area Chair, ACM MM Area Chair.
  - *2024:* General Chair of Inaugural Young Scientists Salon on Artificial Intelligence.
  - *2024:* Associate Editor for IEEE Transactions on Multimedia 2024!
  - *2024:* ECCV Area Chair, WACV Area Chair, program committee of the IJCAI 24 track on AI and Social Good.
  - *2023:* WACV Area Chair
  - *2022:* AAAI SPC, WACV Area Chair
  - *2021:* ACM MM Session Chair, ACM MM Workshop Co-chair
  - *2019:* IJCAI Session Chair, Guest Editor of Journal of Visual Communication and Image Representation, etc.
  - *2018-Now:* Reviewers of IEEE TPAMI, TIP, TMM, TNNLS, TOC; IJCV; CVPR, ECCV, ICCV, AAAI, IJCAI, NeurIPS, ICML, MM, IJCAI and etc.


# üôã Supervisions
<!-- - *2019.05 - 2020.02*, [Lorem](https://github.com/), China.  -->
- **Current Ph.D students:**
  - Kaipeng Fang, Xiao Cai (Enrolled in _Jun. 2023_)
  - Xu Luo, Haonan Zhang (Enrolled in _Jun. 2022_)
  - Hao Ni, Sitong su (Enrolled in _Jun. 2021_)
  - Ji Zhang, Xinyu Lyu, and Juncheng Zhu (Enrolled in _Jun. 2020_)
  
- **Former Ph.D students:**
  - Tao He (Co-supervisor Monash University _Jun.2018 - Nov. 2022_)
    
    _Thesis: Towards Unbiased Scene Graph Generation: Techniques and Applications._
  - Xuanhang Wang (_Jun. 2019 - Jul. 2023_)
  
    _Thesis: Visual semantic understanding based visual dialogue._  
  - Pengpeng Zeng (_Jun. 2019 - Jul. 2023_)
  
    _Thesis: Research on Synergizing Vision and Text for Semantic Consistency Method._
  - Xiangpeng Li (_Jun.2018 - Jul. 2022_)
  
    _Thesis: Research on Visual Reasoning algorithm that integrates natural language analysis._
  - Yuyu Guo (_Jun. 2018 - Jul. 2022_)
  
  _Thesis: Visual Relationship Generation Based on Scene Understanding._

- **Current and former M.Sc. students:**
  - Hilali Sara Rita,Ke Liu, Mengqi Li, Shihan Wu, Fuwei Liu, and Lu Zhu (Enrolled in _Sep. 2022_)
  - Jiaqi Guo, Qisheng Chen, Youheng Sun, Yixin Qin, and Han Wang (Enrolled in _Sep. 2022_)
  - Durasic Aleksandra, Fuchun Wang, and Hao Wu (Enrolled in _Sep. 2021_)
  - Xiaoya Chen, Kai Xing, Jiahui Li, and Wenxue Shen (Graduated _Jun. 2023_)
  - Qike Zhao, Yaya Cheng, and Haoyu Wang (Graduated _Jun. 2022_)
  - Zhilong Zhou, Qian Ye, Hao He, and Ruiming Lang (Graduated _Jun. 2021_)
  - Qingsong Zhang, Liyang Zhang, and Ziming Fang (Graduated _Jun. 2020_)
  - Yuyu Guo (Graduated _Jun. 2019_)
  - Liangfu Cao (Graduated _Jun. 2018_)
  - Chuanshu Long (Graduated _Jun. 2017_)


# üíª Research Grants
 Some selective Research Grants: 
- *2024.01 - 2027.12*, Key Program of National Natural Science Foundation of China: ‚ÄúTrusted Big Cross-Meida Data Analysis and Key Technologies‚Äù, **Lead PI**
- *2022.01 - 2024.12*, Distinguished Young Scholars of the National Natural Science Foundation of China: ‚ÄúVisual Cognition by Integrating Natural Language‚Äù, **Lead PI**.
- *2019.01 - 2022.12*, General Program of National Natural Science Foundation of China: ‚ÄúDeep Visual Understanding by Fusing Natural Language Processing‚Äù, **Lead PI**. 
- *2016.01 - 2018.12*, Young Scientists Fund of the National Natural Science Foundation of China: ‚ÄúDeep Learning and Event Driven based Video Mashup‚Äù, **Lead PI**.

<!-- 
- *2020.12 - 2021.12*, **Lead PI** Fok Ying-Tong Education Foundation of China: ‚ÄúIntegrating Natural Language Processing for multimedia Understanding‚Äù, **Lead PI**.
- *2021.01 - 2024.12*, Ministry of Science and Technology of China, Major Project: ‚ÄúActive Monitoring, Cognition, and Searching for Disaster Environments‚Äù, **PI**.
 -->
