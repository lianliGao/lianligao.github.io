---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

Dr. Lianli Gao(È´òËÅî‰∏Ω) is a Professor in School of Computer Science and Engineering (CSE) at University of Electronic Science and Technology of China (UESTC). Dr. Gao received her Ph.D in School of Electronic Engineering and Computer Science (ITEE) at University of Queensland in 2014 (under the supervision of [Prof. Jane Hunter](https://staff.itee.uq.edu.au/jane/), [Prof. Michael Bruenig](https://researchers.uq.edu.au/researcher/5052) and [A/Prof. Yuan-Fang Li](https://users.monash.edu.au/~yli/) and her B.CS. in School of Computer Science and Engineering (CSE) at University of Electronic Science and Technology of China (UESTC) in 2009, respectively.

Dr. Gao is leader of the Multimedia Analysis and Visual Cognition research group with four Prof. or A/Prof. and three Postdocs. She has a strong ability to develop and author successful grant funding proposals in close collaboration with industry, government partners, and colleagues within and across universities. Her research has been supported by 12 nationally competitive research grants, including one Major Project from the Ministry of Science and Technology of China, two key projects from the National Natural Science Foundation of China, four projects from the industry, etc. In addition, she has served or will serve as ECCV Area Chair 2024, WACV Area Chair 2022-2024, program committee of the IJCAI 24 track on AI and Social Good, AAAI SPC 2022, ACM MM 2021 Session Chair, ACM MM 2021 Workshop Co-chair, IJCAI Session Chair 2019, Guest Editor of 2019 Journal of Visual Communication and Image Representation, etc.

Her research interests include multimedia content understanding, image and video retrieval, vision language, computer vision, and machine learning, and published over 160 publications at prestigious journals and proceedings in prominent conferences (including 70+ IEEE/ACM Transactions and 70+ CCF-A papers (Chinese Computing Federation A ranked (e.g., CVPR, ICCV, NeurIPS, ICML and ICLR)). Her publications have been cited in Google Scholar more than 7,400 times, and her H-Index in Google Scholar is 44.

She has received Best Student Paper Award from Australasian Database Conference 2017, Rising Star Award from IEEE Technical Community on Multimedia Computing 2020, Sichuan Provincial Academic/Technical Leader (Reserve Candidate) 2021, UESTC Research Excellence Award (2018, 2020,2023), Alibaba DAMO Academy Young Fellow Award 2019, and also has been selected as one of the 2023 Chinese Young Female Scholars in Artificial Intelligence for her outstanding academic performance in AI. In addition, in terms of international challenges she received ICCV Deeper Action 3rd Place Award in Kinetics-TPS Challenge on Part-level Action Parsing 2021, CVPR Security AI Challenger Phrase VI Track 1st Place award in White-box Adversarial Attacks on ML Defense Models 2021, ICCV COCO DensePose Challenge 2nd place award 2019, OPPO Security Challenge 2nd Place 2021, and ECCV DeeperAction Track4 3nd Place 2022, etc.

Her Teaching covers a wide range of courses at different levels for both international and national students, including "Introduction to Big Data", "Data Structure and Algorithm", "Introduction to Computer Vision", "Semantic Web", "Experiments of Artificial Intelligence", "Welcome to Academic World" etc. She received very positive feedback (e.g., "nice teacher", "well-organized", "good experience" and "dedicated to providing us with knowledge‚Äú) from her students with a well-deserved 5-star rating. In addition, she has been mentoring junior staff by assessing peer-to-peer teaching performance via attending their classes. Her dedication earned her Excellent Faculty Award for Outstanding Teaching in 2018 and 2023.


# üî• News
<!-- - *2022.02*: &nbsp;üéâüéâ Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  -->
- *2024.02*: &nbsp; Two paper were accepted by IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2024).
- *2023.10*: &nbsp; One paper was accepted by IEEE Transactions on Image Procerssing (TIP 2023).
- *2023.09*: &nbsp; One paper was accepted by Annual Conference on Neural Information Processing Systems (NeurIPS 2023).
- *2023.07*: &nbsp; Four papers were accepted by ACM Multimedia (MM 2023).
- *2023.07*: &nbsp; One paper was accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI 2023).
- *2023.05*: &nbsp; Two papers were accepted by IEEE/CVF International Conference on Computer Vision (ICCV 2023).
- *2023.03*: &nbsp; One paper was accepted by IEEE Transactions on Image Procerssing (TIP 2023).
- *2023.02*: &nbsp; One paper was accepted by IEEE Transactions on Image Procerssing (TIP 2023).
- *2023.01*: &nbsp; One paper was accepted by Pattern Recognition (PR 2021).


# üìù Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/papers/tpami.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Dept: Decoupled prompt tuning](https://arxiv.org/pdf/2309.07439.pdf) \\
J. Zhang, S. Wu, **Lianli Gao**, H. T. Shen, and J. Song.
 
 <a href="https://ieeexplore.ieee,org/stamp/stamp.jsp?arnumber=9969654"><strong>Paper</strong></a>
\|
<a href="https://github.com/Koorye/DePT"><strong>Code</strong></a>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/papers/tpami.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Prog: Prompting-to simulate generalized knowledge for universal cross-domain retrieval](https://arxiv.org/pdf/2312.12478.pdf) \\
K. Fang, J. Song, **Lianli Gao**, P. Zeng, Z.-Q. Cheng, X. Li, and H. T. Shen. 
 
 <a href="https://arxiv.org/pdf/2312.12478.pdf"><strong>Paper</strong></a>
\|
<a href="https://github.com/fangkaipeng/ProS"><strong>Code</strong></a>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IJCV 2024</div><img src='images/papers/tpami.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Informative scene graph generation via debiasing](https://arxiv.org/pdf/2308.05286.pdf) \\
 **Lianli Gao**, X. Lyu, Y. Guo, Y. Hu, Y.-F. Li, L. Xu, H. T. Shen, and J. Song.
 
 <a href="https://arxiv.org/pdf/2308.05286.pdf"><strong>Paper</strong></a>
\|
<a href="https://github.com/ZhuGeKongKong/SGG-G2S"><strong>Code</strong></a>

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TPAMI 2023</div><img src='images/papers/tpami.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Label-guided generative adversarial network for realistic image synthesis](https://users.monash.edu/~yli/assets/pdf/img_syn_tpami2022.pdf) \\
J. Zhu, **Lianli Gao**, J. Song, Y. Li, F. Zheng, X. Li, and H. T. Shen.
 
 <a href="https://users.monash.edu/~yli/assets/pdf/img_syn_tpami2022.pdf"><strong>Paper</strong></a>
\|
<a href="https://github.com/RoseRollZhu/Lab2Pix-V2"><strong>Code</strong></a>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TPAMI 2023</div><img src='images/papers/tpami.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Adaptive fine-grained predicates learning for scene graph generation](https://arxiv.org/pdf/2207.04602.pdf) \\
X. Lyu, Lianli Gao, P. Zeng, H. T. Shen, and J. Song.
 
 <a href="https://arxiv.org/pdf/2207.04602.pdf"><strong>Paper</strong></a>
\|
<a href="https://github.com/XinyuLyu/FGPL"><strong>Code</strong></a>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICML 2023</div><img src='images/papers/cvpr22.png' alt="sym" height="600" width="800"></div></div>
<div class='paper-box-text' markdown="1">

[A closer look at few-shot classification again](https://arxiv.org/pdf/2301.12246.pdf) \\
 X. Luo, H. Wu, J. Zhang, **Lianli Gao**, J. Xu, and J. Song.

<a href="https://arxiv.org/pdf/2301.12246.pdf"><strong>Paper</strong></a>
\|
<a href="https://github.com/Frankluox/CloserLookAgainFewShot"><strong>Code</strong></a>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TIP 2023</div><img src='images/papers/tpami.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Toward a unified transformer-based framework for scene graph generation and human-obfect interaction detection](https://arxiv.org/pf/231101755.pdf) ) \\
T. He, **Lianli Gao**, J. Song, and Y. Li. 
 
 <a href="https://arxiv.org/pdf/2311.01755.pdf""><strong>Paper</strong></a>
\|
<a href="https://lianligao,github.io/"><strong>Code</strong></a>

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2023</div><img src='images/papers/cvpr23.png' alt="sym" width="100%" height="125%"></div></div>
<div class='paper-box-text' markdown="1">

[Prototype-based Embedding Network for Scene Graph Generation](https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_Prototype-Based_Embedding_Network_for_Scene_Graph_Generation_CVPR_2023_paper.pdf) \\
Chaofan Zheng*, **Xinyu Lyu\***(Equal Contribution), Lianli Gao, Bo Dai, Jingkuan Song
<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_Prototype-Based_Embedding_Network_for_Scene_Graph_Generation_CVPR_2023_paper.pdf"><strong>Paper</strong></a>
\|
<a href="https://github.com/VL-Group/PENET"><strong>Code</strong></a>


</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2023</div><img src='images/papers/cvpr23.png' alt="sym" width="100%" height="125%"></div></div>
<div class='paper-box-text' markdown="1">

[Part-aware transformer for generalizable person re-identification](https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_Prototype-Based_Embedding_Network_for_Scene_Graph_Generation_CVPR_2023_paper.pdf) \\
H. Ni, Y. Li, **Lianli Gao**, H. T. Shen, and J. Song.

<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ni_Part-Aware_Transformer_for_Generalizable_Person_Re-identification_ICCV_2023_paper.pdf"><strong>Paper</strong></a>
\|
<a href="https://github.com/liyuke65535/Part-Aware-Transformer"><strong>Code</strong></a>


</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">MM 2023</div><img src='images/papers/cvpr22.png' alt="sym" height="600" width="800"></div></div>
<div class='paper-box-text' markdown="1">

[Moviefactory: Automatic movie creation from text using large generative models for language and images](https://arxiv.org/pdf/2306.07257.pdf) \\
 J. Zhu, H. Yang, H. He, W.Wang, Z. Tuo, W. Cheng, **Lianli Gao**, J. Song, and J. Fu. 

<a href="https://arxiv.org/pdf/2306.07257.pdf"><strong>Paper</strong></a>
\|
<a href="https://lianligao,github.io/"><strong>Code</strong></a>

</div>
</div>



- `ECCV 2022` [State-aware compositional learning toward unbiased training for scene graph generation](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9969654), T. He, **Lianli Gao**, J. Song, and Y. Li. 
- `TIP 2023` [From global to local: Multi-scale out-of-distribution detection](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9969654), J. Zhang, **Lianli Gao**, B. Hao, H. Huang, J. Song, and H. Shen.
- `ICCV 2023` [DETA: denoised task adaptation for few-shot learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_DETA_Denoised_Task_Adaptation_for_Few-Shot_Learning_ICCV_2023_paper.pdf), J. Zhang, **Lianli Gao**, X. Luo, H. Shen, and J. Song. 
- `TIP 2022` [Hierarchical representation network with auxiliary tasks for video captioning and video question answering](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9592722), **Lianli Gao**, Y. Lei, P. Zeng, J. Song, M. Wang, and H. T. Shen.
- `TIP 2022` [Video question answering with prior knowledge and object-sensitive learning](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9882977), P. Zeng, H. Zhang, **Lianli Gao**, J. Song, and H. T. Shen. 
- `CVPR 2022` [Practical evaluation of adversarial robustness via adaptive auto attack](https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Practical_Evaluation_of_Adversarial_Robustness_via_Adaptive_Auto_Attack_CVPR_2022_paper.pdf), Y. Liu, Y. Cheng, **Lianli Gao**, X. Liu, Q. Zhang, and J. Song.
- `CVPR 2022` [Unified multivariate gaussian mixture for efficient neural image compression](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhu_Unified_Multivariate_Gaussian_Mixture_for_Efficient_Neural_Image_Compression_CVPR_2022_paper.pdf), X. Zhu, J. Song, **Lianli Gao**, F. Zheng, and H. T. Shen.
- `CVPR 2022` [Fine-grained predicates learning for scene graph generation](https://openaccess.thecvf.com/content/CVPR2022/papers/Lyu_Fine-Grained_Predicates_Learning_for_Scene_Graph_Generation_CVPR_2022_paper.pdf), X. Lyu, **Lianli Gao**, Y. Guo, Z. Zhao, H. Huang, H. T. Shen, and J. Song.
- `ECCV 2022` [Towards open-vocabulary scene graph generation with prompt-based finetuning](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136880055.pdf), T. He, Lianli Gao, J. Song, and Y. Li. 
- `ECCV 2022` [Frequency domain model augmentation for adversarial attack](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640543.pdf), Long, Q. Zhang, B. Zeng, **Lianli Gao**, X. Liu, J. Zhang, and J. Song.
- `ICLR 2022` [Beyond imagenet attack: Towards crafting adversarial examples for black-box domains](https://arxiv.org/pdf/2201.11528.pdf), Q. Zhang, X. Li, Y. Chen, J. Song, **Lianli Gao**, Y. He, and H. Xue. 
- `NeurIPS 2022` [A differentiable semantic metric approximation in probabilistic embedding for cross-modal retrieval](https://proceedings.neurips.cc/paper_files/paper/2022/file/4e786a87e7ae249de2b1aeaf5d8fde82-Paper-Conference.pdf), H. Li, J. Song, **Lianli Gao**, P. Zeng, H. Zhang, and G. Li. 
- `NeurIPS 2022` [Natural color fool: Towards boosting black-box unrestricted attacks](https://arxiv.org/pdf/2210.02041.pdf), S. Yuan, Q. Zhang, **Lianli Gao**, Y. Cheng, and J. Song.
- `NeurIPS 2022` [A lower bound of hash codes‚Äô performance](https://arxiv.org/pdf/2210.05899.pdf), X. Zhu, J. Song, Y. Lei, **Lianli Gao**, and H. Shen.
- `TPAMI 2020` [Hierarchical lstms with adaptive attention for visual captioning](https://arxiv.org/pdf/1812.11004.pdf), **Lianli Gao**, X. Li, J. Song, and H. T. Shen.
<!-- under review -->

# üéñ Honors and Awards
- *2024.11* Outstanding Graduates of University of Electronic Science and Technology of China.
- *2024.01* SCF Annual Outstanding Student Paper ([Webpage](https://mp.weixin.qq.com/s/n1jSNB4aOd8YiYppOaf2hw))



# üìñ Educations
- *2020.09* - now, Ph.D. student, University of Electronic Science and Technology of China, Chengdu.
- *2017.09* - 2019.06, Master student, Rutgers University, New Jersey. 
- *2014.09* - 2018.06, Undergraduate, University of Electronic Science and Technology of China, Chengdu.



# üí¨ Invited Talks
- *2023.05*, 2023 CVPR Southwest Region Pre-Sharing Session, Chengdu, Sichuan. ([Video](https://wx.vzan.com/live/page/1981937113?v=638188997043420940)) 
- *2022.05*, 2022 CVPR Southwest Region Pre-Sharing Session, Chengdu, Sichuan. ([Video](https://live.bilibili.com/24970653))
- *2022.05*, 2022 ChinaMM (Technical Forum: MM-CV High Impact Paper Appreciation Forum), Guiyang, Guizhou.
- *2019.10*, 2019 ACM MM (Mutual correlation attentive factors in dyadic fusion networks for speech emotion recognition), Nice, France.


# üôã Service
<!-- - *2019.05 - 2020.02*, [Lorem](https://github.com/), China.  -->

<!-- - Journal Reviewer of IEEE Transactions on Knowledge and Data Engineering, IEEE Transactions on Neural Networks and Learning Systems, IEEE Transactions on Systems, Man and Cybernetics: Systems. -->
- **Journal Reviewer**: IEEE TPAMI, IJCV, IEEE TIP, IEEE TCSVT, IEEE TMM, IEEE TGRS, etc.
- **Conference Reviewer**: CVPR, NeurIPS, ACM MM, AAAI, EMNLP, ICPR, ICME, etc.
<!-- - Conference Reviewer: of CVPR 2023/2024, NeurIPS 2022/2023, ACM MM 2021/2023, ICCV 2023, AAAI 2020/2024, EMNLP 2024, ICPR2020. -->
<!-- , CICAI 2021-2022, ICIG 2021, ACML 2021, PRCV 2021-2022 -->
- **Competition Reviewer**: Guangdong-Hong Kong-Macao Greater Bay Area (Whampoa) International Algorithm Calculation Competition 2022 ([Track: Panoptic Scene Graph Genetation](https://www.cvmart.net/race/10349/base?organic_url=https%3A%2F%2Fwww.google.com%2F))/ 2023([Track: FunQA](http://112.74.40.78:9092/contestdetail?id=64af50154a0ed647faca623a&award=1,000,000)) .

# üíª Main Projects
- *2023.02 - 2024.06*, 2030 Science and Technology Innovation Major Project for Next-Generation Artificial Intelligence by the Ministry of Science and Technology, National Key Research and Development Program of China (No. 2018AAA0102200), **Main Researcher**.
- *2021.01 - 2025.12*, Research on Key Technologies for Visual Natural Cognition through Collaborative Vision and Language Processing, National Natural Science Foundation of China under Grant (No. 62020106008), **Participant**.
- *2020.01 - 2022.07*, Research on Scene Graph Generation for E-commerce Live Streaming/Video, Kuaishou (No. 202109FKY00302), **Main Researcher**.
- *2019.01 - 2022.12*, Research on Key Technology Research on Collaborative Deep Video Understanding, Description, and Visual Question Answering, National  Natural  Science Foundation of China (No. 61872064), **Participant**.
- *2018.01 - 2021.12*, Research on Key Technology for Deep Visual Understanding Integrated with Natural Language Processing, National  Natural  Science  Foundation  of  China (No. 61772116), **Participant**.

