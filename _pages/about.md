---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

Dr. Lianli Gao is a Professor in School of Computer Science and Engineering (CSE) at University of Electronic Science and Technology of China (UESTC). Dr. Gao received her Ph.D in School of Electronic Engineering and Computer Science (ITEE) at University of Queensland in 2014 (under the supervision of Prof. Jane HunterÔºå Prof. Michael Bruenig and A/Prof. Yuan-Fang Li and her B.CS. in School of Computer Science and Engineering (CSE) at University of Electronic Science and Technology of China (UESTC) in 2009, respectively.

Dr. Gao is leader of the Multimedia Analysis and Visual Cognition research group with four Prof. or A/Prof. and three Postdocs. She has a strong ability to develop and author successful grant funding proposals in close collaboration with industry, government partners, and colleagues within and across universities. Her research has been supported by 12 nationally competitive research grants, including one Major Project from the Ministry of Science and Technology of China, two key projects from the National Natural Science Foundation of China, four projects from the industry, etc. In addition, she has served or will serve as ECCV Area Chair 2024, WACV Area Chair 2022-2024, program committee of the IJCAI 24 track on AI and Social Good, AAAI SPC 2022, ACM MM 2021 Session Chair, ACM MM 2021 Workshop Co-chair, IJCAI Session Chair 2019, Guest Editor of 2019 Journal of Visual Communication and Image Representation, etc.

Her research interests include multimedia content understanding, image and video retrieval, vision language, computer vision, and machine learning, and published over 160 publications at prestigious journals and proceedings in prominent conferences (including 70+ IEEE/ACM Transactions and 70+ CCF-A papers (Chinese Computing Federation A ranked (e.g., CVPR, ICCV, NeurIPS, ICML and ICLR)). Her publications have been cited in Google Scholar more than 7,400 times, and her H-Index in Google Scholar is 44.

She has received Best Student Paper Award from Australasian Database Conference 2017, Rising Star Award from IEEE Technical Community on Multimedia Computing 2020, Sichuan Provincial Academic/Technical Leader (Reserve Candidate) 2021, UESTC Research Excellence Award (2018, 2020,2023), Alibaba DAMO Academy Young Fellow Award 2019, and also has been selected as one of the 2023 Chinese Young Female Scholars in Artificial Intelligence for her outstanding academic performance in AI. In addition, in terms of international challenges she received ICCV Deeper Action 3rd Place Award in Kinetics-TPS Challenge on Part-level Action Parsing 2021, CVPR Security AI Challenger Phrase VI Track 1st Place award in White-box Adversarial Attacks on ML Defense Models 2021, ICCV COCO DensePose Challenge 2nd place award 2019, OPPO Security Challenge 2nd Place 2021, and ECCV DeeperAction Track4 3nd Place 2022, etc.

Her Teaching covers a wide range of courses at different levels for both international and national students, including "Introduction to Big Data", "Data Structure and Algorithm", "Introduction to Computer Vision", "Semantic Web", "Experiments of Artificial Intelligence", "Welcome to Academic World" etc. She received very positive feedback (e.g., "nice teacher", "well-organized", "good experience" and "dedicated to providing us with knowledge‚Äú) from her students with a well-deserved 5-star rating. In addition, she has been mentoring junior staff by assessing peer-to-peer teaching performance via attending their classes. Her dedication earned her Excellent Faculty Award for Outstanding Teaching in 2018 and 2023.


# üî• News
<!-- - *2022.02*: &nbsp;üéâüéâ Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  -->
- *2023.11*: &nbsp; One paper was accepted by IEEE Transactions on Circuits and Systems for Video Technology (TCSVT 2023).
- *2023.07*: &nbsp; One paper was accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI 2023).
- *2023.07*: &nbsp; One paper was accepted by IEEE Transactions on Circuits and Systems for Video Technology (TCSVT 2023).
- *2023.03*: &nbsp; One paper was accepted by International Symposium on Artificial Intelligence and Robotics (ISAIR 2023).
- *2023.02*: &nbsp; One paper was accepted by IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2023).
- *2022.10*: &nbsp; One paper was accepted by ACM International Conference on Multimedia (ACM MM 2022).
- *2022.07*: &nbsp; Two papers were accepted by IEEE International Conference on Multimedia and Expo (ICME 2022).
- *2022.06*: &nbsp; One paper was accepted by IEEE/CVF Conference on Computer Vision and Pattern Recognitionn (CVPR 2022).
- *2021.10*: &nbsp; One paper was accepted by ACM International Conference on Multimedia (ACM MM 2021).
- *2021.06*: &nbsp; One paper was accepted by Pattern Recognition (PR 2021).
- *2019.10*: &nbsp; One paper was accepted by ACM International Conference on Multimedia (ACM MM 2019).


# üìù Publications 
Full Publications visit Her [Google Scholar](https://scholar.google.com/citations?user=zsm2dpYAAAAJ) and [DBLP](https://dblp.org/pid/123/9849.html).

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TPAMI 2023</div><img src='images/papers/tpami.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Informative scene graph generation via debiasing](https://arxiv.org/pdf/2308.05286.pdf) \\
 **Lianli Gao**, X. Lyu, Y. Guo, Y. Hu, Y.-F. Li, L. Xu, H. T. Shen, and J. Song.
 
 <a href="https://arxiv.org/pdf/2308.05286.pdf"><strong>Paper</strong></a>
\|
<a href="https://github.com/ZhuGeKongKong/SGG-G2S"><strong>Code</strong></a>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/papers/tpami.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Dept: Decoupled prompt tuning](https://arxiv.org/pdf/2309.07439.pdf) \\
J. Zhang, S. Wu, **Lianli Gao**, H. T. Shen, and J. Song.
 
 <a href="https://ieeexplore.ieee,org/stamp/stamp.jsp?arnumber=9969654"><strong>Paper</strong></a>
\|
<a href="https://github.com/Koorye/DePT"><strong>Code</strong></a>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/papers/tpami.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Prog: Prompting-to simulate generalized knowledge for universal cross-domain retrieval](https://arxiv.org/pdf/2309.07439.pdf) \\
J. Zhang, S. Wu, **Lianli Gao**, H. T. Shen, and J. Song.
 
 <a href="https://ieeexplore.ieee,org/stamp/stamp.jsp?arnumber=9969654"><strong>Paper</strong></a>
\|
<a href="https://github.com/Koorye/DePT"><strong>Code</strong></a>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TIP 2023</div><img src='images/papers/tpami.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Toward a unified transformer-based framework for scene graph generation and human-obfect interaction detection](https://ariv org/pf/2311 01755.pdf) ) \\
T. He, **Lianli Gao**, J. Song, and Y. Li. 
 
 <a href="https://arxiv.org/pdf/2311.01755.pdf""><strong>Paper</strong></a>
\|
<a href="https://lianligao,github.io/"><strong>Code</strong></a>

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2023</div><img src='images/papers/cvpr23.png' alt="sym" width="100%" height="125%"></div></div>
<div class='paper-box-text' markdown="1">

[Prototype-based Embedding Network for Scene Graph Generation](https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_Prototype-Based_Embedding_Network_for_Scene_Graph_Generation_CVPR_2023_paper.pdf) \\
Chaofan Zheng*, **Xinyu Lyu\***(Equal Contribution), Lianli Gao, Bo Dai, Jingkuan Song
<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_Prototype-Based_Embedding_Network_for_Scene_Graph_Generation_CVPR_2023_paper.pdf"><strong>Paper</strong></a>
\|
<a href="https://github.com/VL-Group/PENET"><strong>Code</strong></a>


</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2023</div><img src='images/papers/cvpr23.png' alt="sym" width="100%" height="125%"></div></div>
<div class='paper-box-text' markdown="1">

[Part-aware transformer for generalizable person re-identification](https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_Prototype-Based_Embedding_Network_for_Scene_Graph_Generation_CVPR_2023_paper.pdf) \\
H. Ni, Y. Li, **Lianli Gao**, H. T. Shen, and J. Song.

<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ni_Part-Aware_Transformer_for_Generalizable_Person_Re-identification_ICCV_2023_paper.pdf"><strong>Paper</strong></a>
\|
<a href="https://github.com/liyuke65535/Part-Aware-Transformer"><strong>Code</strong></a>


</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2023</div><img src='images/papers/cvpr22.png' alt="sym" height="600" width="800"></div></div>
<div class='paper-box-text' markdown="1">

[DETA: denoised task adaptation for few-shot learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_DETA_Denoised_Task_Adaptation_for_Few-Shot_Learning_ICCV_2023_paper.pdf) \\
 J. Zhang, **Lianli Gao**, X. Luo, H. Shen, and J. Song. 

<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_DETA_Denoised_Task_Adaptation_for_Few-Shot_Learning_ICCV_2023_paper.pdf"><strong>Paper</strong></a>
\|
<a href="https://github.com/JimZAI/DETA"><strong>Code</strong></a>

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICML 2023</div><img src='images/papers/cvpr22.png' alt="sym" height="600" width="800"></div></div>
<div class='paper-box-text' markdown="1">

[A closer look at few-shot classification again](https://arxiv.org/pdf/2301.12246.pdf) \\
 X. Luo, H. Wu, J. Zhang, **Lianli Gao**, J. Xu, and J. Song.

<a href="https://arxiv.org/pdf/2301.12246.pdf"><strong>Paper</strong></a>
\|
<a href="https://github.com/Frankluox/CloserLookAgainFewShot"><strong>Code</strong></a>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">MM 2023</div><img src='images/papers/cvpr22.png' alt="sym" height="600" width="800"></div></div>
<div class='paper-box-text' markdown="1">

[Moviefactory: Automatic movie creation from text using large generative models for language and images](https://arxiv.org/pdf/2306.07257.pdf) \\
 J. Zhu, H. Yang, H. He, W.Wang, Z. Tuo, W. Cheng, **Lianli Gao**, J. Song, and J. Fu. 

<a href="https://arxiv.org/pdf/2306.07257.pdf"><strong>Paper</strong></a>
\|
<a href="https://lianligao,github.io/"><strong>Code</strong></a>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TIP 2022</div><img src='images/papers/cvpr22.png' alt="sym" height="600" width="800"></div></div>
<div class='paper-box-text' markdown="1">

[Hierarchical representation network with auxiliary tasks for video captioning and video question answering](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9592722) \\
 **Lianli Gao**, Y. Lei, P. Zeng, J. Song, M. Wang, and H. T. Shen.

<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9592722"><strong>Paper</strong></a>
\|
<a href="https://github.com/riesling00/HRNAT"><strong>Code</strong></a>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TIP 2022</div><img src='images/papers/cvpr22.png' alt="sym" height="600" width="800"></div></div>
<div class='paper-box-text' markdown="1">

[Video question answering with prior knowledge and object-sensitive learning](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9882977) \\
P. Zeng, H. Zhang, **Lianli Gao**, J. Song, and H. T. Shen. 

<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9882977"><strong>Paper</strong></a>
\|
<a href="https://github.com/zchoi/PKOL"><strong>Code</strong></a>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2022</div><img src='images/papers/cvpr22.png' alt="sym" height="600" width="800"></div></div>
<div class='paper-box-text' markdown="1">

[Practical evaluation of adversarial robustness via adaptive auto attack](https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Practical_Evaluation_of_Adversarial_Robustness_via_Adaptive_Auto_Attack_CVPR_2022_paper.pdf) \\
Y. Liu, Y. Cheng, **Lianli Gao**, X. Liu, Q. Zhang, and J. Song.

<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Practical_Evaluation_of_Adversarial_Robustness_via_Adaptive_Auto_Attack_CVPR_2022_paper.pdf"><strong>Paper</strong></a>
\|
<a href="https://github.com/liuye6666/adaptive_auto_attack"><strong>Code</strong></a>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2022</div><img src='images/papers/cvpr22.png' alt="sym" height="600" width="800"></div></div>
<div class='paper-box-text' markdown="1">

[Unified multivariate gaussian mixture for efficient neural image compression](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhu_Unified_Multivariate_Gaussian_Mixture_for_Efficient_Neural_Image_Compression_CVPR_2022_paper.pdf) \\
Y. Liu, Y. Cheng, **Lianli Gao**, X. Liu, Q. Zhang, and J. Song.

<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhu_Unified_Multivariate_Gaussian_Mixture_for_Efficient_Neural_Image_Compression_CVPR_2022_paper.pdf"><strong>Paper</strong></a>
\|
<a href="https://github.com/xiaosu-zhu/McQuic"><strong>Code</strong></a>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2022</div><img src='images/papers/cvpr22.png' alt="sym" height="600" width="800"></div></div>
<div class='paper-box-text' markdown="1">

[Unified multivariate gaussian mixture for efficient neural image compression](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhu_Unified_Multivariate_Gaussian_Mixture_for_Efficient_Neural_Image_Compression_CVPR_2022_paper.pdf) \\
X. Zhu, J. Song, **Lianli Gao**, F. Zheng, and H. T. Shen.

<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhu_Unified_Multivariate_Gaussian_Mixture_for_Efficient_Neural_Image_Compression_CVPR_2022_paper.pdf"><strong>Paper</strong></a>
\|
<a href="https://github.com/xiaosu-zhu/McQuic"><strong>Code</strong></a>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2022</div><img src='images/papers/cvpr22.png' alt="sym" height="600" width="800"></div></div>
<div class='paper-box-text' markdown="1">

[Fine-grained predicates learning for scene graph generation](https://openaccess.thecvf.com/content/CVPR2022/papers/Lyu_Fine-Grained_Predicates_Learning_for_Scene_Graph_Generation_CVPR_2022_paper.pdf) \\
X. Lyu, **Lianli Gao**, Y. Guo, Z. Zhao, H. Huang, H. T. Shen, and J. Song.

<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Lyu_Fine-Grained_Predicates_Learning_for_Scene_Graph_Generation_CVPR_2022_paper.pdf"><strong>Paper</strong></a>
\|
<a href="https://github.com/XinyuLyu/FGPL"><strong>Code</strong></a>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2022</div><img src='images/papers/cvpr22.png' alt="sym" height="600" width="800"></div></div>
<div class='paper-box-text' markdown="1">

[Towards open-vocabulary scene graph generation with prompt-based finetuning](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136880055.pdf) \\
X. Lyu, **Lianli Gao**, Y. Guo, Z. Zhao, H. Huang, H. T. Shen, and J. Song.

<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136880055.pdf"><strong>Paper</strong></a>
\|
<a href="https://lianligao,github.io/"><strong>Code</strong></a>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2022</div><img src='images/papers/cvpr22.png' alt="sym" height="600" width="800"></div></div>
<div class='paper-box-text' markdown="1">

[Frequency domain model augmentation for adversarial attack](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640543.pdf) \\
Long, Q. Zhang, B. Zeng, **Lianli Gao**, X. Liu, J. Zhang, and J. Song.

<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640543.pdf"><strong>Paper</strong></a>
\|
<a href="https://github.com/yuyang-long/SSA"><strong>Code</strong></a>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2022</div><img src='images/papers/cvpr22.png' alt="sym" height="600" width="800"></div></div>
<div class='paper-box-text' markdown="1">

[Beyond imagenet attack: Towards crafting adversarial examples for black-box domains](https://arxiv.org/pdf/2201.11528.pdf) \\
Q. Zhang, X. Li, Y. Chen, J. Song, **Lianli Gao**, Y. He, and H. Xue. 

<a href="https://arxiv.org/pdf/2201.11528.pdf"><strong>Paper</strong></a>
\|
<a href="https://github.com/Alibaba-AAIG/Beyond-ImageNet-Attack"><strong>Code</strong></a>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2022</div><img src='images/papers/cvpr22.png' alt="sym" height="600" width="800"></div></div>
<div class='paper-box-text' markdown="1">

[A differentiable semantic metric approximation in probabilistic embedding for cross-modal retrieval](https://proceedings.neurips.cc/paper_files/paper/2022/file/4e786a87e7ae249de2b1aeaf5d8fde82-Paper-Conference.pdf) \\
H. Li, J. Song, **Lianli Gao**, P. Zeng, H. Zhang, and G. Li. 

<a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/4e786a87e7ae249de2b1aeaf5d8fde82-Paper-Conference.pdf"><strong>Paper</strong></a>
\|
<a href="https://github.com/leolee99/2022-NeurIPS-DAA"><strong>Code</strong></a>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2022</div><img src='images/papers/cvpr22.png' alt="sym" height="600" width="800"></div></div>
<div class='paper-box-text' markdown="1">

[Natural color fool: Towards boosting black-box unrestricted attacks](https://arxiv.org/pdf/2210.02041.pdf) \\
S. Yuan, Q. Zhang, **Lianli Gao**, Y. Cheng, and J. Song.

<a href="https://arxiv.org/pdf/2210.02041.pdf"><strong>Paper</strong></a>
\|
<a href="https://github.com/ylhz/Natural-Color-Fool"><strong>Code</strong></a>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2022</div><img src='images/papers/cvpr22.png' alt="sym" height="600" width="800"></div></div>
<div class='paper-box-text' markdown="1">

[A lower bound of hash codes‚Äô performance](https://arxiv.org/pdf/2210.05899.pdf) \\
X. Zhu, J. Song, Y. Lei, **Lianli Gao**, and H. Shen.

<a href="https://arxiv.org/pdf/2210.05899.pdf"><strong>Paper</strong></a>
\|
<a href=" https://github.com/VL-Group/LBHash"><strong>Code</strong></a>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TPAMI 2020</div><img src='images/papers/cvpr22.png' alt="sym" height="600" width="800"></div></div>
<div class='paper-box-text' markdown="1">

[Hierarchical lstms with adaptive attention for visual captioning](https://arxiv.org/pdf/1812.11004.pdf) \\
**Lianli Gao**, X. Li, J. Song, and H. T. Shen.

<a href="https://arxiv.org/pdf/1812.11004.pdf"><strong>Paper</strong></a>
\|
<a href="https://github.com/lixiangpengcs/Spatial-Temporal-Adaptive-Attention-for-Video-Captioning"><strong>Code</strong></a>

</div>
</div>




# üéñ Honors and Services
- 2023, 2020, 2018 UESTC Research Excellence Award.
- 2023, 2018 UESTC Excellent Faculty Award for Teaching Excellence.
- 2023 Chinese Young Female Scholars in Artificial Intelligence.
- 2021 Sichuan Provincial Academic/Technical Leader (Reserve Candidate).
- 2020 IEEE Transactions on Multimedia Best Paper Award.
- 2020 IEEE Technical Community on Multimedia Computing Rising Star Award.
- 2019 Alibaba DAMO Academy Young Fellow Award.
- 2017 Australasian Database Conference Best Student Paper Award.
- Grand Challnges:
-   ECCV2022 DeeperAction Challenge 3rd place award on Track 4 Kinetics-TPS Challenge on Part-level Action Parsing.
-   CVPR2021 Security AI Challenger Phrase VI Track 1st Place award in White-box Adversarial Attacks on ML Defense Models.
-   ICCV2021 DeeperAction Challenge 3rd Place award on Track 3 Kinetics-TPS Challenge on Part-level Action Parsing.
-   OPPO2021 Security Challenge 2nd Place award.
-   ICCV 2019 COCO DensePose Task Challenge 2nd place award.
- Adademic Services:
-   2024: ECCV Area Chair, WACV Area Chair, program committee of the IJCAI 24 track on AI and Social Good.
-   2023: WACV Area Chair
-   2022: AAAI SPC, WACV Area Chair
-   2021: ACM MM Session Chair, ACM MM Workshop Co-chair
-   2019: IJCAI Session Chair, Guest Editor of Journal of Visual Communication and Image
Representation, etc.


# üôã Supervision
<!-- - *2019.05 - 2020.02*, [Lorem](https://github.com/), China.  -->
- **Current Ph.D students:**
  - Kaipeng Fang, Xiao Cai (Enrolled in Jun. 2023)
  - Xu Luo, Haonan Zhang (Enrolled in Jun. 2022)
  - Hao Ni, Sitong su (Enrolled in Jun. 2021)
  - Ji Zhang, Xinyu Lyu and Juncheng Zhu (Enrolled in Jun. 2020)
  
- **Former Ph.D students:**
  - Tao He (Co-supervisor Monash University Jun.2018 - Nov. 2022)
    
    _Thesis: Towards Unbiased Scene Graph Generation: Techniques and Applications._
  - Xuanhang Wang (Jun. 2019 - Jul. 2023)
  
    _Thesis: Visual semantic understanding based visual dialogue._	
  - Pengpeng Zeng (Jun. 2019 - Jul. 2023)
  
    _Thesis: Research on Synergizing Vision and Text for Semantic Consistency Method._
  - Xiangpeng Li (Jun.2018 - Jul. 2022)
  
    _Thesis: Research on Visual Reasoning algorithm that integrates natural language analysis._
  - Yuyu Guo (Jun. 2018 - Jul. 2022)
	
	_Thesis: Visual Relationship Generation Based on Scene Understanding._


- **Current and former M.Sc. students:**
  - Hilali Sara Rita,Ke Liu, Mengqi Li, Shihan Wu, Fuwei Liu, and Lu Zhu (Enrolled in Sep. 2022)
  - Jiaqi Guo, Qishen Chen, Youhen Sun, Yixin Qin, and Han Wang (Enrolled in Sep. 2022)
  - Durasic Aleksandra, Fuchun Wang, and Hao Wu (Enrolled in Sep. 2021)
  - Xiaoya Chen, Kai Xing, Jiahui Li, and Wenxue Shen(Graduated Jun. 2023)
  - Qike Zhao, Yaya Cheng, and Haoyu Wang (Graduated Jun. 2022)
  - Zhilong Zhou, Qian Ye, Hao He, and Ruiming Lang (Graduated Jun. 2021)
  - Qingsong Zhang, Liyang Zhang, and Ziming Fang (Graduated Jun. 2020)
  - Yuyu Guo (Graduated Jun. 2019)
  - Liangfu Cao (Graduated Jun. 2018)
  - Chuanshu Long (Graduated Jun. 2017)

# üíª Research Grants
- **Lead PI**, *2024.01 - 2027.12*, Key Program of National Natural Science Foundation of China: ‚ÄúTrusted Big Cross-Meida Data Analysis and Key Technologies‚Äù.
- **Lead PI**, *2023.06 - 2024.07*, Industry Funds of Kwai Technology: ‚ÄúLarge Scale Open-set Multimedia Retrieval‚Äù.
- **Lead PI**, *2022.01 - 2024.12*, Distinguished Young Scholars of the National Natural Science Foundation of China: ‚ÄúVisual Cognition by Integrating Natural Language‚Äù.
- **Lead PI**, *2021.01 - 2024.12*, Ministry of Science and Technology of China, Major Project: ‚ÄúActive Monitoring, Cognition, and Searching for Disaster Environments‚Äù.
- **Lead PI**, *2020.12 - 2021.12*, Fok Ying-Tong Education Foundation of China: ‚ÄúIntegrating Natural Language Processing for multimedia Understanding‚Äù.
- **Lead PI**, *2019.01 - 2022.12*, General Program of National Natural Science Foundation of China: ‚ÄúDeep Visual Understanding by Fusing Natural Language Processing‚Äù. 
- **Lead PI**, *2016.01 - 2018.12*, Young Scientists Fund of the National Natural Science Foundation of China: ‚ÄúDeep Learning and Event Driven based Video Mashup‚Äù.


